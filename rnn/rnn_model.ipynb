{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import glob\n",
    "#from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import preprocessing as pp\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:42:53.960539485Z",
     "start_time": "2024-01-07T12:42:53.692819120Z"
    }
   },
   "id": "daa48e61d9b31fa3"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "length = 5\n",
    "n_features=3\n",
    "max_embed_length=500\n",
    "out_cat_embed_dim=4 # This may can be increased, default for screen2words: 2\n",
    "out_nested_embed_dim=4 # This may should be increased, as this is a heavy dimension reduction\n",
    "label_length=1\n",
    "training_frames = length + label_length\n",
    "batch_size = 32\n",
    "dropout_fraction = 0.2\n",
    "\n",
    "train_test_split = 0.25 # 1/4 the data is for testing\n",
    "dataset_share = 0.2 # Should be 1.0 in production"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:42:54.030786436Z",
     "start_time": "2024-01-07T12:42:53.929945923Z"
    }
   },
   "id": "55d7839ec1e2833a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_features_from_gestures(gesture_path):\n",
    "  with tf.io.gfile.GFile(gesture_path) as f:\n",
    "    gesture_json = json.load(f)\n",
    "  trace_path = gesture_path.replace('gestures.json', '')\n",
    "  # Throw away all gestures except the first\n",
    "  gesture_items = list(gesture_json.items())\n",
    "  # Remove empty gestures\n",
    "  gesture_items = [gesture for gesture in gesture_items if gesture[1]]\n",
    "  # TODO: Add column: isSwipe, if gesture has more than one entry\n",
    "  if gesture_items and gesture_items[0]:\n",
    "    gesture_dict = {gesture[0]:gesture[1][0] for gesture in gesture_items}\n",
    "  else:\n",
    "    gesture_dict = {}\n",
    "\n",
    "  first_gesture_item = list(gesture_dict.items())\n",
    "  if (not gesture_dict) or (not first_gesture_item[0]) or (not first_gesture_item[0][1]):\n",
    "    # Return empty dataframe, if no data is available\n",
    "    return pd.DataFrame(columns=['x', 'y']), trace_path\n",
    "  \n",
    "  gesture_df = pd.DataFrame(gesture_dict).transpose()\n",
    "  gesture_df.columns = ['x', 'y']\n",
    "  return gesture_df, trace_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:42:54.121242615Z",
     "start_time": "2024-01-07T12:42:54.024090184Z"
    }
   },
   "id": "370e5596e68c7cc9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "'../sources/datasets/RICO/traces/filtered_traces/*/trace_*/gestures.json'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traces_prefix = \"../sources/datasets/RICO/traces/filtered_traces\"\n",
    "path_to_gesture_json = f\"{traces_prefix}/*/trace_*/gestures.json\"\n",
    "path_to_gesture_json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:42:54.157284874Z",
     "start_time": "2024-01-07T12:42:54.102870910Z"
    }
   },
   "id": "1010903aaff8e289"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "10292"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gesture_json_paths = glob.glob(path_to_gesture_json)\n",
    "len(gesture_json_paths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:42:59.733022160Z",
     "start_time": "2024-01-07T12:42:54.134584468Z"
    }
   },
   "id": "3d96e858f6638676"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_features_from_tree(frame_index, trace_path):\n",
    "  file_prefix = f\"{trace_path}view_hierarchies/{frame_index}\"\n",
    "  features_dict = pp.create_simple_features(file_prefix)\n",
    "  if features_dict is None:\n",
    "    return None\n",
    "  del features_dict['node_id']\n",
    "  feature_items = list(features_dict.items())\n",
    "  #feature_items = to_var_len_feature_dict(feature_items).items()\n",
    "  tree_feature_df = pd.DataFrame(feature_items, columns=['feat', frame_index])\n",
    "  tree_feature_df = tree_feature_df.set_index('feat')\n",
    "  tree_feature_df = tree_feature_df.transpose()\n",
    "  return tree_feature_df\n",
    "  #return pd.DataFrame(features_adapt, index=[frame_index])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:42:59.735457769Z",
     "start_time": "2024-01-07T12:42:59.721860524Z"
    }
   },
   "id": "897083222d435531"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "feat                                        obj_dom_pos  \\\n391   [1, 1, 7, 2, 2, 6, 2, 6, 2, 2, 7, 1, 3, 3, 5, ...   \n\nfeat            type_id_seq         visibility_seq visibility_to_user_seq  \\\n391   [4, 4, 4, 4, 4, 4, 4]  [1, 1, 1, 1, 0, 1, 1]  [1, 1, 1, 1, 0, 1, 1]   \n\nfeat          clickable_seq                     cord_x_seq_start  \\\n391   [0, 0, 0, 0, 0, 0, 1]  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n\nfeat                       cord_x_seq_end  \\\n391   [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]   \n\nfeat                                   cord_y_seq_start  \\\n391   [0.0, 0.0, 0.934375, 0.0, 0.0, 0.0328125, 0.03...   \n\nfeat                                     cord_y_seq_end  \n391   [1.0, 0.934375, 1.0, 0.0328125, 0.0, 0.934375,...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>feat</th>\n      <th>obj_dom_pos</th>\n      <th>type_id_seq</th>\n      <th>visibility_seq</th>\n      <th>visibility_to_user_seq</th>\n      <th>clickable_seq</th>\n      <th>cord_x_seq_start</th>\n      <th>cord_x_seq_end</th>\n      <th>cord_y_seq_start</th>\n      <th>cord_y_seq_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>391</th>\n      <td>[1, 1, 7, 2, 2, 6, 2, 6, 2, 2, 7, 1, 3, 3, 5, ...</td>\n      <td>[4, 4, 4, 4, 4, 4, 4]</td>\n      <td>[1, 1, 1, 1, 0, 1, 1]</td>\n      <td>[1, 1, 1, 1, 0, 1, 1]</td>\n      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]</td>\n      <td>[0.0, 0.0, 0.934375, 0.0, 0.0, 0.0328125, 0.03...</td>\n      <td>[1.0, 0.934375, 1.0, 0.0328125, 0.0, 0.934375,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gesture_path = gesture_json_paths[43]\n",
    "gesture_df, trace_path = get_features_from_gestures(gesture_path)\n",
    "frameIndex = gesture_df.index.values[0]\n",
    "feature_df = get_features_from_tree(frameIndex, trace_path)\n",
    "feature_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:42:59.845343426Z",
     "start_time": "2024-01-07T12:42:59.722067939Z"
    }
   },
   "id": "3fe910aceba99bcd"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "            x         y                                        obj_dom_pos  \\\n391  0.515094  0.732075  [1, 1, 7, 2, 2, 6, 2, 6, 2, 2, 7, 1, 3, 3, 5, ...   \n\n               type_id_seq         visibility_seq visibility_to_user_seq  \\\n391  [4, 4, 4, 4, 4, 4, 4]  [1, 1, 1, 1, 0, 1, 1]  [1, 1, 1, 1, 0, 1, 1]   \n\n             clickable_seq                     cord_x_seq_start  \\\n391  [0, 0, 0, 0, 0, 0, 1]  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]   \n\n                          cord_x_seq_end  \\\n391  [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]   \n\n                                      cord_y_seq_start  \\\n391  [0.0, 0.0, 0.934375, 0.0, 0.0, 0.0328125, 0.03...   \n\n                                        cord_y_seq_end  \n391  [1.0, 0.934375, 1.0, 0.0328125, 0.0, 0.934375,...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x</th>\n      <th>y</th>\n      <th>obj_dom_pos</th>\n      <th>type_id_seq</th>\n      <th>visibility_seq</th>\n      <th>visibility_to_user_seq</th>\n      <th>clickable_seq</th>\n      <th>cord_x_seq_start</th>\n      <th>cord_x_seq_end</th>\n      <th>cord_y_seq_start</th>\n      <th>cord_y_seq_end</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>391</th>\n      <td>0.515094</td>\n      <td>0.732075</td>\n      <td>[1, 1, 7, 2, 2, 6, 2, 6, 2, 2, 7, 1, 3, 3, 5, ...</td>\n      <td>[4, 4, 4, 4, 4, 4, 4]</td>\n      <td>[1, 1, 1, 1, 0, 1, 1]</td>\n      <td>[1, 1, 1, 1, 0, 1, 1]</td>\n      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]</td>\n      <td>[0.0, 0.0, 0.934375, 0.0, 0.0, 0.0328125, 0.03...</td>\n      <td>[1.0, 0.934375, 1.0, 0.0328125, 0.0, 0.934375,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feat_df = pd.concat([gesture_df, feature_df], axis=1)\n",
    "all_feat_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:42:59.847828120Z",
     "start_time": "2024-01-07T12:42:59.756884848Z"
    }
   },
   "id": "a60861c0e824008e"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#feature_df['type_id_seq'].iloc[0][0].dtype"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-07T12:42:59.850004587Z",
     "start_time": "2024-01-07T12:42:59.759882450Z"
    }
   },
   "id": "924f3487fba88fba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 7384 has no feature datatraces.\n",
      "Processed 3500 of 10292 (34%) traces.\r"
     ]
    }
   ],
   "source": [
    " # Get set of frames for each screen, but don't mix them during training LSTM\n",
    "feature_traces = pd.DataFrame()\n",
    "use_embeddings = True\n",
    "\n",
    "trace_progress = 0\n",
    "for trace_id, gesture_path in enumerate(gesture_json_paths):\n",
    "    trace_progress = trace_progress + 1\n",
    "    if trace_progress % 100 == 0:\n",
    "      print(f'Processed {trace_progress} of {len(gesture_json_paths)} ({trace_progress/len(gesture_json_paths):.0%}) traces.\\r', end=\"\")\n",
    "    gesture_df, trace_path = get_features_from_gestures(gesture_path)\n",
    "    # Traces with too few frames will be dropped\n",
    "    if len(gesture_df) < training_frames:\n",
    "        #print(f'Gesture (traceID: {trace_id}) has only {len(gesture_df)} of {training_frames} required frames: {gesture_path}')\n",
    "        continue\n",
    "    if use_embeddings: # Try without features for now, only gestures\n",
    "        tree_features_df = pd.DataFrame()\n",
    "        for frame_id in gesture_df.index.values:\n",
    "            tree_feature_df = get_features_from_tree(frame_id, trace_path)\n",
    "            if tree_feature_df is None:\n",
    "              print(f'Frame {frame_id} has no feature data')\n",
    "              continue\n",
    "            tree_features_df = pd.concat([tree_features_df, tree_feature_df], axis=0)\n",
    "        trace_features_df = pd.concat([gesture_df, tree_features_df], axis=1)\n",
    "    else:\n",
    "        trace_features_df = gesture_df\n",
    "    trace_features_df['trace_id'] = trace_id\n",
    "    feature_traces = pd.concat([feature_traces, trace_features_df], axis=0)\n",
    "\n",
    "len(feature_traces)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-01-07T12:42:59.769213276Z"
    }
   },
   "id": "288b287670f0c8ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_traces.head()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f671b3c1a1ee0b7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_traces.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c6025a04827454b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gestures_only = False # Model only uses gestures / clicks, to determine the next click.\n",
    "correct_gesture_shift = True # Shifts the gesture / click sequence one to the right, as the gesture of the current screen should be predicted, not the one on the next screen\n",
    "\n",
    "# TODO set different input_dim for each feature\n",
    "embedding_specs = [\n",
    "  # feat_name, is_cat, input_dim, output_dim, # maybe needed: vocab_size\n",
    "  ('x', False, 1, None, None),\n",
    "  ('y', False, 1, None, None),\n",
    "]\n",
    "\n",
    "if not gestures_only:\n",
    "  cat_embedding_specs = [\n",
    "    ('cord_x_seq_start', False, max_embed_length, None, None),\n",
    "    ('cord_x_seq_end', False, max_embed_length, None, None),\n",
    "    ('cord_y_seq_start', False, max_embed_length, None, None),\n",
    "    ('cord_y_seq_end', False, max_embed_length, None, None),\n",
    "    ('obj_dom_pos', True, max_embed_length, out_cat_embed_dim, 255),\n",
    "    ('type_id_seq', True, max_embed_length, out_cat_embed_dim, 10),\n",
    "    ('visibility_seq', False, max_embed_length, None, None),\n",
    "    ('visibility_to_user_seq', False, max_embed_length, None, None),\n",
    "    ('clickable_seq', False, max_embed_length, None, None),\n",
    "    # ('obj_dom_pos_2', True, max_embed_length, out_cat_embed_dim),\n",
    "  ]\n",
    "  embedding_specs.extend(cat_embedding_specs)\n",
    "  \n",
    "considered_features = [specs[0] for specs in embedding_specs]\n",
    "considered_cat_features = [specs[0] for specs in embedding_specs if specs[1]]\n",
    "\n",
    "print(considered_features)\n",
    "print(considered_cat_features)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e144a7ffb986fc5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_traces_preprocessed = feature_traces\n",
    "print(len(feature_traces_preprocessed))\n",
    "feature_traces_preprocessed = feature_traces_preprocessed.dropna()\n",
    "feature_traces_preprocessed = feature_traces_preprocessed[considered_features + ['trace_id']]\n",
    "print(len(feature_traces_preprocessed))\n",
    "feature_traces_preprocessed.head()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ff3c0bf8c6dda573"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "\n",
    "# Preprocess data\n",
    "# for t,v in keys_to_dtype.items():\n",
    "#   # try to convert float to list\n",
    "#   feature_traces_preprocessed[t] = feature_traces_preprocessed[t].apply(lambda x : [np.nan if (y is None or isinstance(y, float)) else y for y in x] if isinstance(x, list) else [x])\n",
    "# \n",
    "# #lens of list\n",
    "# print(feature_traces_preprocessed.isna().sum())\n",
    "# feature_traces_preprocessed\n",
    "\n",
    "# List of median value count for each feature.\n",
    "feat_median_array_length = {}\n",
    "\n",
    "# Check if feature is an array\n",
    "isFeatList = (feature_traces_preprocessed.map(type) == list).all(axis='rows')\n",
    "max_dom_pos = 500 # see screen2words\n",
    "\n",
    "def print_feat_infos(feat_name, isList):\n",
    "  print(f'Handle Feature \"{feat_name}\":')\n",
    "  print(f'Shape of Feature \"{np.array(feature_traces_preprocessed[feat_name]).shape}\":')\n",
    "  if isList:\n",
    "    lengths_of_feature = feature_traces_preprocessed[feat_name].map(len)\n",
    "    # FIXME: check why header is given as first row\n",
    "    # print(lengths_of_feature)\n",
    "    # for f_length in lengths_of_feature:\n",
    "    #   if not isinstance(f_length, float):\n",
    "    #     print(f_length)\n",
    "    try:\n",
    "      median_value = statistics.median(lengths_of_feature)\n",
    "      feat_median_array_length[feat_name] = median_value\n",
    "      print(f'\\thas median array length of {median_value}')\n",
    "      print(f'\\thas max array length of {max(lengths_of_feature)}')\n",
    "    except:\n",
    "      print(f'Failed to process lengths:\\n {lengths_of_feature}')\n",
    "      raise\n",
    "  \n",
    "    # Remove all NaN inner values (?): TODO check!!!\n",
    "    feature_traces_preprocessed[feat_name] = feature_traces_preprocessed[feat_name].map(lambda row: [x for x in row if not math.isnan(x)])\n",
    "  \n",
    "    # Extract the maximum value:\n",
    "    allvals = [x for xs in feature_traces_preprocessed[feat_name] for x in xs]\n",
    "    print(f'\\thas median value of {statistics.median(allvals)}')\n",
    "    print(f'\\thas max value of {max(allvals)}')\n",
    "  \n",
    "    # Pad all values to same length\n",
    "    # TODO need to pad to higher values per input\n",
    "    feat_pad = keras.utils.pad_sequences(feature_traces_preprocessed[feat_name], value=0, padding='post', maxlen=max_dom_pos)\n",
    "    feature_traces_preprocessed[feat_name] = feat_pad.tolist()\n",
    "  else:\n",
    "    print(f'\\tis single value only')\n",
    "\n",
    "for feat_name, isList in isFeatList.items():\n",
    "  print_feat_infos(feat_name, isList)\n",
    "\n",
    "feat_median_array_length"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "96079ecf4c9fe3a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# feature_traces_flatten = feature_traces_preprocessed\n",
    "# for feat_name, isList in isFeatList.items():\n",
    "#   if isList:\n",
    "#     max_len = len(feature_traces_flatten[feat_name])\n",
    "#     feature_traces_flatten = feature_traces_flatten.drop(feat_name, axis= 1)\n",
    "#     feature_traces_flatten[[f'{feat_name}_{i}' for i in max_len]] = pd.DataFrame(feature_traces_preprocessed[feat_name].tolist(), index= feature_traces_preprocessed.index)\n",
    "#     #feature_traces_preprocessed = feature_traces_preprocessed + feature_traces_preprocessed.teams.apply(pd.Series)\n",
    "#     #df3.columns = ['team1', 'team2']"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9386cd8d843f63b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_traces_preprocessed.info()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4518fcc7b8bbdd90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The data is split by trace and not by frame windows as otherwise the model would be trained with similar apps before\n",
    "feature_traces_list = [pd.DataFrame(x[1]).drop('trace_id', axis=1) for x in feature_traces_preprocessed.groupby('trace_id')]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2e5c1681d1fcf7f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Shift click sequence by one to predict click of current screen\n",
    "def shift_click_sequence(trace_df):\n",
    "  trace_df['x'] = trace_df['x'].shift(1, fill_value=0.5) # Choose 0.5 to use the screen center as entry value\n",
    "  trace_df['y'] = trace_df['y'].shift(1, fill_value=0.5)\n",
    "  return trace_df\n",
    "  \n",
    "if correct_gesture_shift:\n",
    "  feature_traces_list = list(map(shift_click_sequence,feature_traces_list))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1e04c8cfb26cebe6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "feature_traces_list[0]\n",
    "# len(feature_traces_preprocessed[0].iloc[0]['obj_dom_pos'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1d11d0c674f86a3c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: add validation split and use test split only to evaluation\n",
    "\n",
    "all_size = len(feature_traces_list)\n",
    "print(f'All size: {all_size}')\n",
    "share_size = math.floor(all_size * dataset_share)\n",
    "print(f'Share size: {share_size}')\n",
    "test_size = math.floor(share_size * train_test_split)\n",
    "test_ind = share_size - test_size\n",
    "print(f'Train size: {test_ind}')\n",
    "print(f'Test size: {test_size}')\n",
    "\n",
    "train_traces = feature_traces_list[:test_ind]\n",
    "test_traces = feature_traces_list[test_ind:share_size]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "68dc568c059d2160"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#df['obj_dom_pos'].dtype"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e7971c3e9fc1ce6c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.layers import Normalization\n",
    "\n",
    "isScale = False # Currently all values are already scaled, as far as known\n",
    "if isScale:\n",
    "  scaler = Normalization()\n",
    "  scaler_invert = Normalization(invert=True)\n",
    "  \n",
    "  print('Adapt scaler')\n",
    "  for train in train_traces:\n",
    "    scaler.adapt(train[['x','y']])\n",
    "    scaler_invert.adapt(train[['x','y']])\n",
    "  \n",
    "  print('Scale train data')\n",
    "  scaled_train_traces = train_traces\n",
    "  for train in train_traces[['x','y']]:\n",
    "    scaled_train_traces[['x','y']] = scaler(train)\n",
    "  \n",
    "  print('Scale test data')\n",
    "  scaled_test_traces = test_traces\n",
    "  for test in test_traces[['x','y']]:\n",
    "    scaled_test_traces[['x','y']] = scaler(test)\n",
    "  \n",
    "  print(scaler_invert(scaled_train_traces[0]))\n",
    "else:\n",
    "  scaled_train_traces = train_traces\n",
    "  scaled_test_traces = test_traces\n",
    "  "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3ef65237c3f7c11e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaled_train_traces[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "95f3bafe5ec03b33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "# # Replace by timeseries_dataset_from_array\n",
    "# generator = TimeseriesGenerator(scaled_train, scaled_train, length=length, batch_size=1)\n",
    "# validation_generator = TimeseriesGenerator(scaled_test,scaled_test, length=length, batch_size=1)\n",
    "# len(generator)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b6de01b5523b9891"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X,y = generator[0]\n",
    "# X"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bb6fb4ed4cb81857"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert to list of dicts\n",
    "scaled_train_traces = [v.to_dict(orient='records') for v in scaled_train_traces]\n",
    "scaled_test_traces = [v.to_dict(orient='records') for v in scaled_test_traces]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7b4c73197d04e7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_fake_dataset(length=5):\n",
    "  my_arr = []\n",
    "  for i in range(0, length):\n",
    "    my_dict = {\n",
    "      'x': np.random.uniform(0, 1),\n",
    "      'y': np.random.uniform(0, 1),\n",
    "      'obj_dom_pos': np.random.randint(0, 255, size=max_embed_length),\n",
    "    }\n",
    "    my_arr.append(my_dict)\n",
    "  return my_arr\n",
    "\n",
    "override_with_fake = False\n",
    "if override_with_fake:\n",
    "  scaled_train_traces = [create_fake_dataset(30) for i in range(0, 3)]\n",
    "  scaled_test_traces = [create_fake_dataset(20) for i in range(0, 3)]\n",
    "\n",
    "scaled_train_traces[0]\n",
    "#for x in scaled_train_traces[0]:\n",
    "#  print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "269d16a30b66233b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "319001e7ee3658dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def make_dataset_from_generator(data):\n",
    "  generator = TimeseriesGenerator(data, data, length=length, batch_size=1)\n",
    "  # shuffle=False, # TODO: Shuffle if possible\n",
    "  return generator\n",
    "\n",
    "# def reduce_generators(a,b):\n",
    "#   a = list(map(lambda a1: a1, a))\n",
    "#   b = list(map(lambda b1: b1, b))\n",
    "#   return np.concatenate((a, b))\n",
    "\n",
    "train_generators = list(map(make_dataset_from_generator,scaled_train_traces))\n",
    "validation_generators = list(map(make_dataset_from_generator,scaled_test_traces))\n",
    "print(f'#train generators: {len(train_generators)}')\n",
    "print(f'#test generators: {len(validation_generators)}')\n",
    "generator = train_generators[0]\n",
    "validation_generator = validation_generators[0]\n",
    "generator[0]\n",
    "# train_dataset = reduce(reduce_generators, train_generators)\n",
    "# print(train_dataset)\n",
    "\n",
    "# print(generator)\n",
    "# X,y = generator[0]\n",
    "# print('X')\n",
    "# print(X)\n",
    "# print('y')\n",
    "# print(y)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "360c3e4384131914"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense,LSTM,Embedding, Input, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4d041342072cad7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator[0][1]#[0][0]#['y'][:]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "33887580cc2f711e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def merge_generators(generators):\n",
    "  generators = [[v for v in g] for g in generators]\n",
    "  return reduce(lambda a,b: a + b, generators)\n",
    "\n",
    "def transform_generators_to_tuple(generators):\n",
    "  timeseries = merge_generators(generators)\n",
    "  # TODO: find out, why these arrays are nested in [0] and [0,0]\n",
    "  return [xY[0][0] for xY in timeseries], [xY[1][0] for xY in timeseries]\n",
    "\n",
    "# Filter out a feature from a time window to its own array\n",
    "def filter_var(window, feat_name):\n",
    "  return [step[feat_name] for step in window]\n",
    "\n",
    "# def filter_var(generator, feat_name):\n",
    "#   return [[[filter_var_row(row, feat_name) for row in g[0]],filter_var_row(g[1], feat_name)] for g in generator]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "67076612bea0dab2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_X, train_y = transform_generators_to_tuple(train_generators)\n",
    "print(len(train_X))\n",
    "print(len(train_y))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2e3135a8f3b8f65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_X[0]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4883cb88bb5899bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_y[0]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5eba9fdb5c24e586"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_X, test_y = transform_generators_to_tuple(validation_generators)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5fb8d4cb043429b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def transform_window_to_input(window, feat_name, is_list):\n",
    "  if is_list:\n",
    "    return filter_var(window, feat_name)\n",
    "  else:\n",
    "    feat_vals = filter_var(window, feat_name)\n",
    "    feat_vals = np.expand_dims(feat_vals, axis=1) # Bring to same shape as cats, just wrap the inner value in an array\n",
    "    return feat_vals\n",
    "\n",
    "# Transform list of windows of features to list of features of windows\n",
    "def transform_X_to_inputs(X):\n",
    "  inputs = []\n",
    "  for feat_name, is_cat, input_dim, output_dim, vocab_dim in embedding_specs:\n",
    "    feat_input = [transform_window_to_input(window, feat_name, is_list=input_dim > 1) for window in X]\n",
    "    inputs.append(np.array(feat_input))\n",
    "  return inputs\n",
    "\n",
    "def transform_y_to_labels(y):\n",
    "  labels_pos_x = filter_var(y, 'x')\n",
    "  labels_pos_y = filter_var(y, 'y')\n",
    "  return [np.array(labels_pos_x), np.array(labels_pos_y)]\n",
    "\n",
    "train_inputs = transform_X_to_inputs(train_X)\n",
    "train_labels = transform_y_to_labels(train_y)\n",
    "\n",
    "test_inputs = transform_X_to_inputs(test_X)\n",
    "test_labels = transform_y_to_labels(test_y)\n",
    "\n",
    "for inp in train_inputs:\n",
    "  print(inp.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "51cc731cef710231"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/52627739/how-to-merge-numerical-and-embedding-sequential-models-to-treat-categories-in-rn/52629902#comment136040845_52629902"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "3ed7eb195e0350a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "numerical_inputs = []\n",
    "num_embedded = []\n",
    "cat_inputs = []\n",
    "cat_embedded = []\n",
    "for i, (feat_name, is_cat, input_length, out_cat_embed_dim, vocab_dim) in enumerate(embedding_specs):\n",
    "  print(i, feat_name, is_cat, input_length, out_cat_embed_dim)\n",
    "  if is_cat:\n",
    "    # We have multiple inputs for the same category\n",
    "    input = Input(shape=(length, input_length), name=feat_name)\n",
    "    cat_inputs.append(input)\n",
    "    embed = Embedding(input_dim=vocab_dim, output_dim=out_cat_embed_dim, mask_zero=True)(input)\n",
    "    # Reshape embedding layer to be flattened:\n",
    "    # we have `input_dim` values for this category, which should each be embedded in its own column\n",
    "    # the third dimension is used as embedding dimension, which is shared with the float values dimension\n",
    "    feat_embed_dim = input_length * out_cat_embed_dim\n",
    "    embed = keras.layers.Reshape((length, feat_embed_dim))(embed)\n",
    "    # TODO: play around with multiple DENSE and DROPOUT layers to reduce values of each category, which can be extremely high!! But also to raise performance.\n",
    "    if feat_embed_dim > out_nested_embed_dim:\n",
    "      embed = Dense(out_nested_embed_dim)(embed)\n",
    "    cat_embedded.append(embed)\n",
    "  elif input_length > 1:\n",
    "    input = Input(shape=(length, input_length), name=feat_name)\n",
    "    if input_length > out_nested_embed_dim:\n",
    "      embed = Dense(out_nested_embed_dim)(input)\n",
    "    else:\n",
    "      embed = input\n",
    "    numerical_inputs.append(input)\n",
    "    num_embedded.append(embed)\n",
    "  else:\n",
    "    input = Input(shape=(length, input_length), name=feat_name)\n",
    "    numerical_inputs.append(input)\n",
    "    num_embedded.append(input)\n",
    "\n",
    "cat_merged = []\n",
    "num_merged = []\n",
    "if len(cat_embedded) > 0:\n",
    "  cat_merged = keras.layers.concatenate(cat_embedded)\n",
    "  merged = cat_embedded\n",
    "if len(num_embedded) > 0:\n",
    "  num_merged = keras.layers.concatenate(num_embedded)\n",
    "  merged = num_merged\n",
    "if (len(cat_embedded) > 0) and (len(num_embedded) > 0):\n",
    "  merged = keras.layers.concatenate([num_merged, cat_merged])\n",
    "elif (len(cat_embedded) == 0) and (len(num_embedded) == 0):\n",
    "  merged = keras.layers.concatenate(numerical_inputs)\n",
    "\n",
    "# Consider adding `stateful=True` and `model.reset_states()` to keep interpreting windows as continous batch e.g. for one app.\n",
    "# See: https://stackoverflow.com/a/50235563/5164462\n",
    "out = LSTM(128, return_sequences=False)(merged)\n",
    "# TODO: play with Dense layers and dropout layers\n",
    "out = Dense(32)(out)\n",
    "out = Dropout(dropout_fraction)(out)\n",
    "# Can remove the dense layer, if want to predict the whole screen\n",
    "out = Dense(2)(out)\n",
    "\n",
    "model = Model(numerical_inputs + cat_inputs, out)\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "34dd685643cc59ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss',patience=4)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5ddbd2551425ba6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(\n",
    "  train_inputs,\n",
    "  train_labels,\n",
    "  epochs=100,\n",
    "  validation_data=(test_inputs,test_labels),\n",
    "  callbacks=[early_stop],\n",
    "  batch_size=batch_size\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5153ee85bd325762"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# For clicks only:\n",
    "# loss: 0.0793\n",
    "# val_loss: 0.0872\n",
    "# 7 epochs, until early stop\n",
    "\n",
    "# With features, but clicks not shifted:\n",
    "# loss: 0.0824\n",
    "# val_loss: 0.0948\n",
    "# 9 epochs until early stop"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "21f70f3b1882567e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = pd.DataFrame(model.history.history)\n",
    "loss. index += 1\n",
    "loss.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c77bfad7c65a7364"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Evaluate on Test Data"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a719850fbb1ea4b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaled_test_traces_lengths = list(map(len, scaled_test_traces))\n",
    "# Get the longest trace\n",
    "longest_scaled_test_trace = max(scaled_test_traces_lengths)\n",
    "print(f'Length of longest test trace: {longest_scaled_test_trace}')\n",
    "index_longest_scaled_test_trace = scaled_test_traces_lengths.index(max(scaled_test_traces_lengths))\n",
    "print(f'Index of longest test trace: {index_longest_scaled_test_trace}')\n",
    "\n",
    "scaled_test_traces[105]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "37aa4f8bb92ceaf7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "my_prediction_trace = index_longest_scaled_test_trace\n",
    "scaled_test = scaled_test_traces[my_prediction_trace]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2e5d39e9e24960b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "\n",
    "first_eval_batch = scaled_test[:length] # Use first batch from a test set and predict the next value\n",
    "first_eval_batch = [first_eval_batch] # Must be wrapped in an array to represent shape of X (here: only one entry)\n",
    "first_eval_batch = transform_X_to_inputs(first_eval_batch)\n",
    "current_batch = first_eval_batch\n",
    "\n",
    "print(current_batch)\n",
    "\n",
    "# Of course cannot predict more than one step, as we only predict gesture_pos_x and gesture_pos_y and not the whole tree\n",
    "# for i in range(len(scaled_test)):\n",
    "# \n",
    "#   # get prediction 1 time stamp ahead ([0] is for grabbing just the number instead of [array])\n",
    "#   current_pred = model.predict(current_batch)[0]\n",
    "# \n",
    "#   # store prediction\n",
    "#   test_predictions.append(current_pred)\n",
    "#   \n",
    "#   # drop first value\n",
    "#   current_batch = [feat_input[:,1:,:] for feat_input in current_batch]\n",
    "# \n",
    "#   # update batch to now include prediction\n",
    "#   current_batch = np.append(current_batch,[[current_pred]],axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9b1ca8a355e04809"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Current prediction only\n",
    "current_pred = model.predict(current_batch)\n",
    "print(current_pred)\n",
    "test_predictions.append(current_pred[0]) # Model only predicts one gesture per batch, this can be \"easily\" changed by handing over a batch of labels during training and setting `return_sequences` to true\n",
    "test_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a397607d6f3bc2ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_predictions = np.array(test_predictions)\n",
    "test_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ca20633d1fee164c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "next_values = scaled_test[length:length + 1] # Remove the first batch and just select the next one, as we don't predict more right now\n",
    "next_labels = transform_y_to_labels(next_values)\n",
    "next_labels = np.transpose(next_labels, (1,0))\n",
    "next_labels"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d4765d69f35307f3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if isScale:\n",
    "  # TODO: validate\n",
    "  true_predictions_labels = scaler_invert(test_predictions)\n",
    "  true_next_labels = scaler_invert(next_labels)\n",
    "else:\n",
    "  true_predictions_labels = test_predictions\n",
    "  true_next_labels = next_labels\n",
    "\n",
    "comparison = np.concatenate([true_next_labels, true_predictions_labels], axis=1)\n",
    "comparison_df = pd.DataFrame(comparison, columns=['x','y','PredictionsX', 'PredictionsY'])\n",
    "comparison_df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "aa1ec2a553956e07"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "comparison_df.plot(color = ['#00FF00', '#FF0000', '#00FF88', '#FF0088'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "be71c392c2c63323"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(np.sqrt(mean_squared_error(comparison_df['x'],comparison_df['PredictionsX'])))\n",
    "print(np.sqrt(mean_squared_error(comparison_df['y'],comparison_df['PredictionsY'])))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d9c81d675a439b2b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "comparison_2d = np.concatenate([true_next_labels, true_predictions_labels], axis=0)\n",
    "comparison_2d = pd.DataFrame(comparison_2d, columns=['x', 'y'])\n",
    "sns.scatterplot(x='x', y='y', data=comparison_2d)\n",
    "# comparison_2d.plot.scatter(x='x', y='y', c='DarkBlue')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "171736bad9e0c6e1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "32f41c558af41d85"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
