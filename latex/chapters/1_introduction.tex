\chapter{Introduction}
\label{sec:introduction}

This is a typical human-computer interaction thesis structure for an introduction which is structured in four paragraphs as follows:
% First Paragraph
% CORE MESSAGE OF THIS PARAGRAPH:

\section{The role of Intent Prediction}
\label{sec:role-intent-prediction}

\todo{Explain intent prediction}

What and why
Specifically, intent is the “immediate reason, purpose, or goal” that motivates a user to query a search engine.

Gestures / click sequences are only a small fraction of the user intent.
But it can be easily replaced by more semantic data, which might even be easier to predict.
Descriptive User intent embedding as preliminary stage, which can be applied with Screen2Words\cite{screen2words} in a similar fashion.
\cite{kofler2016user}

\section{Necessity of Vectors for Android UI}
\label{sec:necessity-of-vectors-for-android-ui}
\todo{Explain how many time a user spents on a mobile device, facilitate steps, make it more productive, qualitative user experience}


Motivation for transforming Android UI tree data to vectors

- open source code for predicting next user click / action
- evaluate and compare existing approaches
- provide tools to reduce screen sequences to vectors
- how to work with multidimensional (multi-modal) data in RNNs

- Low Button depth: number of clicks until one gets to the action \cite{lee2018click}

- Explain intent, and the other words in the title

Contributions: \cite{zhou2021large}
• An analysis of a large-scale dataset of mobile user click se-
quences that reveals rich factors and complexity in modeling
click behaviors, which contributes new knowledge to under-
stand mobile interaction behaviors.
• A Transformer-based deep model that predicts next element
to click based on the user click history and the current screen
and time. The model does not rely on a vocabulary of prede-
fined UI elements and provides a general solution for model-
ing arbitrary UI elements for click prediction.
• A thorough experiment that compares our deep model with
multiple alternative designs and baseline models, and an
analysis of model behaviors and benefits that the model can
bring to improve mobile interaction.

Contributions: \cite{li2021screen2vec}
Screen2Vec: a new self-supervised technique for generating
more comprehensive semantic embeddings of GUI screens
and components using their textual content, visual design
and layout patterns, and app meta-data.
(2) An open-sourced GUI embedding model trained using the
Screen2Vec technique on the Rico [9] dataset that can be
used off-the-shelf.
(3) Several sample downstream tasks that showcase the model’s
usefulness.

\label{subsec:motivation}
\todo{P1.1. What is the large scope of the problem?}
\todo{P1.2. What is the specific problem?}

% Second Paragraph
% CORE MESSAGE OF THIS PARAGRAPH:
\todo{P2.1. The second paragraph should be about what have others been doing}
\todo{P2.2. Why is the problem important? Why was this work carried out?}

% Third Paragraph
% CORE MESSAGE OF THIS PARAGRAPH:
\todo{P3.1. What have you done?}
\todo{P3.2. What is new about your work?}

% Fourth paragraph
% CORE MESSAGE OF THIS PARAGRAPH:
\todo{P4.1. What did you find out? What are the concrete results?}
\todo{P4.2. What are the implications? What does this mean for the bigger picture?}

%LaTeX hints are provided in \autoref{chap:latexhints}.
