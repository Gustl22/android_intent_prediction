\chapter{Theoretical Framework}
\label{ch:theoretical-framework}

Before starting with rehabilitate the main topic, some basic concepts, technical terms and underlying theory has to be explained.
This is important to faster read through the thesis without the need to interrupt the flow of thought.
In the following, a set of established theories and terms is explained which are not directly related to the thesis formulation, but essential to comprehend numerous contexts.

\section{Android UI}
\label{sec:android-ui}

To be able to use any of the things displayed on the mobile device, some concepts of \gls{ui} programming must be shown.
A \gls{ui} enables the user to view the applications data on the screen but also to interact with the device especially on mobile devices \cite{android_ui_layer}.

The main challenge is to bring the application data in the right format, so that the display can interpret the instructions to draw the elements.
Each mobile phone with the Android \gls{os} has a basic set of native functions through which the \gls{ui} can be drawn and updated, a so-called \gls{api}.
These functions can be very general to instruct drawing a whole component such as an alert box, or they can be very specific as drawing single rectangles in a canvas.

The rough transition from the application data (data layer) to the display data (ui layer) is depicted in figure \ref{fig:android_udf}.
The application data is transformed, concatenated or filtered to be saved in a view model, which represents the state for each view.
The view model is then layed out to multiple \gls{ui} elements.
E.g.\ they are loaded in the Android activity via view layouts~\cite{android_draw_views} or composed in a declarative approach~\cite{android_jetpack_compose}.
It is generally advised to use a \gls{udf}.
This ensures that the data is only changed in one place and doesn't get out of sync between \gls{ui} elements, the view model and the data layer.
The \gls{ui} can also register user inputs (like a button press) and report them back to the view model.
The view model then updates the application data, if needed, and then also reports the current \gls{ui} state back to the UI elements to be rerendered.

\begin{figure}[htbp!]
    \centering
    \begin{subfigure}[b]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/android_udf}
        \caption{Diagram of \gls{udf} in app architecture \cite{android_ui_layer}}
        \label{fig:android_udf}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{graphics/android_semantics-ui-tree}
        \caption{Schema of how the semantics tree is related to the \gls{ui} hierarchy \cite{android_semantics_compose}}
        \label{fig:android_semantics_ui_tree}
    \end{subfigure}
    \caption{Structure of the Android \gls{ui}}
    \label{fig:android_tree}
\end{figure}
%https://developer.android.com/guide/topics/ui/how-android-draws \cite{android_draw_views}
%https://developer.android.com/jetpack/compose/mental-model \cite{android_jetpack_compose}

\subsection{Data tree structure}
\label{subsec:data-tree-structure}

The \gls{ui} elements (i.e.\ the composition) themselves are hierarchically structured in a tree.
This allows the renderer to calculate relative distances, floatings and skip processing hidden or overlapped elements.

With the Android Layout Inspector (figure~\ref{fig:android_layout_inspector}) a view hierarchy tree can be visually inspected while displaying its position and layout on the Android screen.
Also, the layout attributes can be validated.
This tool allows to debug complex \gls{ui}s especially when using nested components and display them in a simplistic way.
Note that this tool is only available if one has access to the app's source code.

\begin{figure}[htbp!]
    \centering
    \includegraphics[width=\textwidth]{graphics/android_layout_inspector}
    \caption{The Android Layout Inspector tool using the example of \quotes{App ins Grüne}~\cite{mimuc_app_ins_gruene} by the Media Informatics Group of the LMU in Munich.}
    \label{fig:android_layout_inspector}
\end{figure}

\subsection{Android Accessibility Service}
\label{subsec:android-accessibility-service}

If an app is running in production on a users device, meaning that the app is compiled and publicly available, the ways of accessing the Android \gls{ui} tree are limited.
This behavior is of course wanted for safety and privacy reasons.
Nonetheless, if desired, a user can explicitly allow certain apps to gain access to the semantics tree of your Android \gls{os}.
This is especially useful for providing accessibility services for impaired users (like done with the TalkBack app).
Or -- as in our case -- the setting can be used to enable services which collect data for user studies or scientific experiments.

This semantics tree is deviated from the existing \gls{ui} tree.
It can be fed via special semantic properties while composing the \gls{ui}, e.g.\ by specifying the \code{contentDescription} property of an icon \cite{android_semantics_compose}.
Providing semantics is not limited to native platforms as shown by Flutter~\cite{flutter_semantics} and React Native~\cite{react_native_accessibility}.
In figure~\ref{fig:android_semantics_ui_tree} a schema is presented, which shows how the elements of the semantics tree are spanned compared to the components on the \gls{ui} layer.

%https://github.com/android/codelab-android-accessibility
To take advantage of the semantics tree, a custom accessibility service can be built, which can run in the background.
This service tracks all UI changes and has access to the current view hierarchy of the screen, which also inherits the semantic tree.
By altering the code of the \ti{AccessibilityNodeInfoDumper} one can extract the view hierarchy to a locale or remote database~\cite{android_accessibility_node_info_dumper}.
In the code section \ref{android_accessibility_node} a small fraction of a view hierarchy is shown.
It contains nested nodes with various attributes which represent the components of the combined \gls{ui} and semantics hierarchy.

%Semantics tree:
%https://api.flutter.dev/flutter/widgets/Semantics-class.html
%https://developer.android.com/jetpack/compose/semantics
%https://android.googlesource.com/platform/frameworks/testing/+/jb-dev/uiautomator/library/src/com/android/uiautomator/core/AccessibilityNodeInfoDumper.java \cite{android_accessibility_node_info_dumper}
%https://github.com/Gustl22/android-accessibility/blob/c158808533d6fc017455184a7317555d3e6946f6/GlobalActionBarService/app/src/main/java/com/example/android/globalactionbarservice/uiautomator/AccessibilityNodeInfoDumper.java

%Mean 18 actionable elements, with Std=12. \cite{zhou2021large}

\lstinputlisting[language=XML,label=android_accessibility_node,caption={Android Accessibility Node in XML.},float]{code/android_accessibility_node.xml}

\section{Machine Learning}
\label{sec:machine-learning}

\gls{ml}, a term spread by Arthur Lee Samuel, is a method of data analysis, more precisely a scientific approach to form statistical models without the need to explicitly program it~\cite{mahesh2020machine}.
It uses algorithms to iteratively learn how data is structured.
In contrast to statistical inference or manually crafted statistical models respectively, \gls{ml} can solve tasks by automation of model building.
Its advantages lie in finding hidden relations and patterns from the context, without having any or only a small pre knowledge of the data, thus it is a strong tool for generalization or abstraction of large datasets, also known as \gls{gl-bigdata}.
\gls{ml} can be applied to the following fields among others: email and spam filtering, fraud detection, cybersecurity, web search engines, recommender systems (like known from Netflix or Amazon), advertising, translators and text generation, pattern and image recognition.
The data driven approach also comes with some drawbacks: the outcome heavily depends on the provided data.
It can include biases and therefore may acquire forms of discrimination or unfair treatment.
Nonetheless \gls{ml} has a lot of potential to uncover hidden connections in large datasets.


\todo{explain Tensors, Datasets}
%https://stackoverflow.com/a/48599383/5164462

\subsection{Preprocessing}
\label{subsec:preprocessing}

Preprocessing describes the step after one acquired their data, but before training the \gls{ml} model.
This step is not to be underestimated.
A \gls{ml} model can perform significantly better when certain preprocessing steps are applied \cite{alam2019impact}.

To be able to preprocess, we have to know with what kind of data we handle with.
Data entries can occur in different forms, but we can break them down in three main types:
\begin{itemize}
    \item \tb{Categorical values}: a value is always assigned to a class with a fixed pool of predetermined classes.
          E.g.\ letters, words, brands, animals, chemical elements
    \item \tb{Continuous values}: the value can be fractional and may lies in between a lower and an upper bound.
          E.g.\ temperature, velocity, geographic position
    \item \tb{Integer values}: the value is a whole number and may also lie in between a lower and an upper bound.
          E.g.\ revolutions per minute, product number, annual sales
\end{itemize}

For all types -- discrete, continuous, and categorical values -- we have a wide variety of options for preparing them in order to be subsequently processed by a \gls{ml} model \cite{duong2021}.


\subsubsection{Feature selection}

Feature selection is a crucial step to successfully develop a model with the desired results.
It can improve learning performance, thus reduces time, increases computational efficiency, decreases memory storage, and helps build better generalization models \cite{li2017feature}.
Also, it may be a valid approach to get around missing data and can help structure the data by removing unnecessary clutter.

%https://dl-acm-org.emedien.ub.uni-muenchen.de/doi/pdf/10.1145/3136625
On the technical sight, feature selection can be differentiated for two goals: supervised and unsupervised learning~\cite{li2017feature}.
However, principles from the supervised feature selection can be applied in the unsupervised domain, resulting in a semi-supervised \ref{semi-supervised} filter selection.
For classifiers and regression problems (unsupervised) following methods can be applied.
Multiple features can be compared by calculating their correlation.
If one feature is uncorrelated to all other features, this may be an indicator that this feature can be dropped, as it doesn't contribute to the resulting model (\ti{filter method}).
However, this can only be stated for linear correlations, thus it can contribute to the result in an unpredicted way.
Also, if two features correlate too much to each other, one feature probably is redundant and can be dropped.
A good approach is also to reduce the dimensionality of the input data, e.g.\ by replacing~\gls{gl-one-hot} encoded features of the same domain with embeddings (\ti{embedded method}).
This is also called \ti{feature extraction}.
Feature extraction can also be used in unsupervised feature selection.
Clustering is a common approach to reduce the number of input dimensions by gaining insights of which classes can be merged and which need to stay.
A more computational but promising solution is to filter the features by optimizing the model result, also called a \ti{wrapper method}.
By gradually removing and adding features and calculating the models performance, one can determine which inputs are important and which are at risk to increase computing time without noticeable effect.

\todo{formulate or remove}
semantic or legal feature selection
Such as Filtering privacy invasive details
Remove sensitive data
%https://news.mit.edu/2023/new-way-look-data-privacy-0714

Parameterizing the vectorization process
a) Vector length
b) Weighting of features
c) Manipulating individual parameters of model

\subsubsection{Missing data}
Some data entries may are missing.
Therefore, you have two approaches to get around these missing values.
One can drop these values by removing the column or row.
This is only recommended if you are not relying on this data entry, or this the whole feature is not expected to be important enough to bring any value to the model's performance.
Further you can fill the data with a default value like zero or calculate a reasonable value from the surrounding data entries by taking their \quotes{mean, median, or interpolation}~\cite{duong2021}.
The second approach can only be applied to numerical data.

\subsubsection{Normalization and Standardization}

This is only applicable for numerical data.
Many \gls{ml} models work better or exclusively with normalized data.
This means that the values have to be in a certain range, most commonly are from \tb{0} to \tb{1} or from \tb{-1} to \tb{1}.
This can be achieved by dividing all values with the difference of the minimum and maximum value and shift the output accordingly \cite{duong2021}.

% X new = (X — X min)/ (X max — X min)

Sometimes this is not enough, e.g.\ if having a few extreme values, and an approach is desired which better reflects the average data.
Here the standardization, also called z-score normalization, comes into play.
This method scales the values so that the mean value is placed at \tb{0} and the standard deviation is placed at \tb{1}.
%https://medium.com/analytics-vidhya/what-are-data-standardization-and-data-normalization-f880dd9e79b6
\subsubsection{Padding}
\label{subsubsec:padding}

Padding is a technique to adapt input sequences or matrices of different dimensions to the same size.
For example if one screen only has five \gls{ui} elements, and the next one has twenty, the input size varies significantly.
Therefore, there exists three approaches to overcome this problem~\cite{baeldung_padding}.
The first one is to extend all inputs of a specific feature to the longest available input dimension of this feature.
Then every sequence has to be filled with additional values, almost always \tb{0} is used for that, until it matches the longest dimension (\ti{same padding}).
Another technique called \ti{valid padding} cuts all data values after reaching the smallest dimension of all inputs of the feature.
One can also use a combination to e.g.\ pad all inputs with zero to the mean dimension, but throw away all exceeding values as they aren't expected to contribute to a better result.
\ti{Causal padding} is a form of \ti{same padding}, but it prepends the fill values in front of the sequence.
This is helpful if having \gls{rnn}s, which rely on a lengthier inputs in order to predict their next value, without need of filtering out short sequences.

\subsubsection{Categorical Values and One-Hot-Encoding}
\label{subsubsec:categorical_variables}

\todo{cite}

A categorical value must be treated differently than numerical ordinal values.
This is demonstrated best on a concrete example.
Imagine a dataset with pictures of animals, and we want to categorize their species.
Then the values of all possible species like \code{dog} and \code{cat} is called \ti{vocabulary}.
To write it in a table, one could add a column called \code{species} and write their species as a \ti{string} (cf.\ table~\ref{tab:raw_data_table}).
Unfortunately a \gls{ann} cannot work with \ti{strings}, but only with numerical values.
So one could think that mapping each species to a number can solve this issue.
Indeed, this is possible for ordinal categories, which have a strict linear order, such as ratings or gradings.
They are then treated the same as \tb{integer values}.
But animals aren't structured ordinal, thus each species must be treated equally.
To reflect that we split the species column in multiple sub-columns, so that each new one represents one species, see table~\ref{tab:one-hot}.
If the animal belongs to that species, one inserts a \tb{1} or \tb{True} and if not, a \tb{0} or \tb{False} is filled.
This is also called \gls{gl-one-hot}-encoding.

\begin{table}[htbp!]
    \begin{subtable}[c]{0.5\textwidth}
        \centering
        \begin{tabular}{|l|l|l|}
            \hline
            \tb{$I_{d}$} & \textbf{Img} & \textbf{Species} \\
            \hline
            0 & \ti{blob} & cat \\
            1 & \ti{blob} & dog \\
            2 & \ti{blob} & cat \\
            3 & \ti{blob} & horse \\
            \hline
        \end{tabular}
        \subcaption{Raw data table}
        \label{tab:raw_data_table}
    \end{subtable}
    \begin{subtable}[c]{0.5\textwidth}
        \centering
        \begin{tabular}{|l|l|l|l|l|}
            \hline
            \tb{$I_{d}$} & \tb{Img} & \tb{Cat} & \tb{Dog} & \tb{Horse}\\
            \hline
            0 & \ti{blob} & 1 & 0 & 0 \\
            1 & \ti{blob} & 0 & 1 & 0 \\
            2 & \ti{blob} & 1 & 0 & 0 \\
            3 & \ti{blob} & 0 & 0 & 1 \\
            \hline
        \end{tabular}
        \subcaption{One-hot encoded species}
        \label{tab:one-hot}
    \end{subtable}
    \caption{\Gls{gl-one-hot}-encoding of categories, $I_{d}$ is the index of the data entry}
    \label{tab:cat_one_hot}
\end{table}

\subsection{Types of Machine Learning}

\cite{ghahramani2003unsupervised}

\subsubsection{Supervised Learning}
Supervised: Classification and regression
Uses \tb{labeled} examples: Input and output is known

Learns by comparison of the output it is provided with the output the model \ti{predicts}.

Steps:
- Data acquisition
- Data cleaning / Preprocessing (Panadas)
- Split into Training Data, Validation data, and Test data (cannot adapt the model after using the test data)
- Train the model with the train data
- Evaluate the model with the test data, then can adapt the model by the developer
- Last deploy the model to production


\tb{Semi-supervised Learning}

The algorithms using semi-supervised learning seek to adopt from unlabeled and labeled data~\cite{van2020survey}.
It can be assumed that the model is trained first with labeled data, e.g.\ to be able to differentiate certain clusters.
Then the model predicts the labels of the unlabeled data which is then serving as new data point to train the model.
This helps to enlarge the cluster or move its boundaries.
%Test points to be classified by the model into the clusters then have a smaller distance to the previous predicted point,
%and therefore adapting the label of it's cluster, instead of being assigned to a different cluster.
It can be seen as a model trained with classification through labeled data and then extended with clustering through unlabeled data.
Also, if having only few labeled points, one can start with training a cluster through unlabeled data and then determine the class label of the clusters with the labeled data points.

\tb{Self-supervised Learning}

\tb{Reinforcement Learning}
Can also be applied to LSTM

\subsubsection{Unsupervised Learning}
Clustering
Dimensionality reduction

\subsection{Under and Overfitting}
\label{subsec:under-and-overfitting}


\section{Artificial Neural Nets}
\label{sec:artificial-neural-nets}

- Uses biological neuron systems as paradigm to generate mathematical models
- can solve tasks by abstraction or generalization of data relations


Activation Functions
Cost function
Gradient
- Regression: Continuous Values
- Classification: Multiple class
- One Class

\subsection{Classes of Neural Nets}
\label{subsec:classes-of-neural-nets}

\subsubsection{Deep Neural Nets}

Neural Net with more than one layer
- Dense Layer

\subsubsection{Convolutional Neural Nets}
\subsubsection{Recurrent Neural Networks and LSTMs / GRU}
supervised

LSTM 4 dimensional
% https://stackoverflow.com/questions/54743549/is-it-possible-to-making-lstm-model-with-4-dimension-shape-of-data

Limitations to only 3 dimensions, needs flattening

Sample dimension (X -> y)
Time (Step) Dimension
Feature Dimension
Data, Quantity dimension, such as Image dimensions, or multiple nodes

TimeDistributedLayer
% https://stackoverflow.com/a/61588937/5164462
% https://stackoverflow.com/questions/53107126/what-are-the-uses-of-timedistributed-wrapper-for-lstm-or-any-other-layers

\subsubsection{Dense Layer}
\label{subsubsec:dense-layer}

\subsubsection{Embedding Layer}
\label{subsubsec:embedding-layer}

% https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/
As mentioned in section~\ref{subsubsec:categorical_variables} categories can be represented using \gls{gl-one-hot} encoding.
The conversion can result in innumerable amount of columns, which is equivalent each having its own feature dimension.
To reduce the dimensionality of such encoding, so-called categorical \ti{embeddings} can be introduced~\cite{brownlee2021}.
This means that each single species is represented by a vector.
The length of the vector can be selected freely, it is called the \ti{embedding dimension}.
A lookup-table is created which encodes each category with randomly initialized weights of size of the embedding dimension (cf.\ table~\ref{tab:species-embedding}).
Now the embedding vector for each species is looked up and replaced in the data table resulting in an embedded representation (cf.\ table~\ref{tab:embedding_data_table}).
During training of the model the weights of the look-up table are successively updated (like for the dense layer ~\ref{subsubsec:dense-layer}) to reduce the loss of the overall model.
An embedding would result in an \gls{gl-one-hot}-encoding, if the embedding dimension and the vocabulary size are equal and the conversion matrix (lookup-table) is the identity matrix.

\begin{table}[htbp!]
    \begin{subtable}[c]{0.5\textwidth}
        \centering
        \begin{tabular}{|l|l|l|l|}
            \hline
            \tb{$I_{d}$} & \tb{Img} & \tb{SP\_1} & \tb{SP\_2}\\
            \hline
            0 & \ti{blob} & 0.1 & 0.6 \\
            1 & \ti{blob} & 0.4 & 0.8 \\
            2 & \ti{blob} & 0.1 & 0.6 \\
            3 & \ti{blob} & 0.5 & 0.5 \\
            \hline
        \end{tabular}
        \subcaption{Species encoded with embedding}
        \label{tab:embedding_data_table}
    \end{subtable}
    \begin{subtable}[c]{0.5\textwidth}
        \centering
        \begin{tabular}{|l|l|l|l|}
            \hline
            \tb{Species} & \tb{$I_{s}$} & \tb{SP\_1} & \tb{SP\_2}\\
            \hline
            0 & cat & 0.1 & 0.6 \\
            1 & dog & 0.4 & 0.8 \\
            2 & horse & 0.5 & 0.5 \\
            \hline
        \end{tabular}
        \subcaption{Lookup table for the 2-dimensional embedding of species}
        \label{tab:species-embedding}
    \end{subtable}
    \caption{Embedding of categories, $I_{d}$ is the index of the data entry, $I_{s}$ is the index of the species}
    \label{tab:cat_embeddings}
\end{table}



According to \cite{alam2019impact} these steps can be removal of emoticons, elimination of stopwords and stemming for text based models.

Category Embedding before LSTM
% https://stackoverflow.com/questions/47217151/keras-lstm-with-embedding-layer-before-lstm-layer
% https://stackoverflow.com/questions/52627739/how-to-merge-numerical-and-embedding-sequential-models-to-treat-categories-in-rn/52629902#comment136040845_52629902
% https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work
% https://stackoverflow.com/questions/47868265/what-is-the-difference-between-an-embedding-layer-and-a-dense-layer


Embedding dimension is about the actual voc\_size, but not too large.
Dimension near the actual average length of features (?)

\subsubsection{Autoencoders}
\label{subsubsec:autoencoder}

Encoder, Decoder

\subsection{Tensorflow and Keras}
Layers
FlattenLayer

Positive Integer to Dense Vectors of fixed size

\section{Evaluation and Metrics}
\subsection{Mean Squared Error}

\subsection{\texorpdfstring{F\textsubscript{1}-Score}{F1-Score}}
\label{subsec:f1-score}

\begin{comment}
Die F\textsubscript{1}-Wertung ist eine der am häufigst verwendeten Metriken zur Evaluation von \acrshort{ml}-Modellen. Um den F\textsubscript{1}-Wert berechnen zu können, werden die Werte der im vorherigen Kapitel beschriebenen Confusion Matrix benötigt. Die Formel zur Berechnung des F\textsubscript{1}-Wertes lautet:

\begin{equation}
    F\textsubscript{1} = (\frac{\text{1/Precision + 1/Recall}}{2})\textsuperscript{-1}
\end{equation}

Dazu werden die Formeln für den Wert von \textit{Precision} und \textit{Recall} benötigt.

\begin{equation}
    Precision = \frac{\text{\acrshort{tp}}}{\text{\acrshort{tp} + \acrshort{fp}}}
\end{equation}
\begin{equation}
    Recall = \frac{\text{\acrshort{tp}}}{\text{\acrshort{tp} + \acrshort{fn}}}
\end{equation}

Der Wert der Precision-Gleichung wird durch den Anteil an richtig vorhergesagten positiven
Ergebnissen (\acrshort{tp}) auf die Gesamtheit aller als positiv vorhergesagten Ergebnissen berechnet. Mit der Recall-Gleichung, welche auch Hit-Rate genannt wird, wird mit der Anzahl
\acrshort{tp}-Ergebnisse geteilt durch die Gesamtheit der tatsächlich positiven Ergebnisse,
der Wert der Gleichung berechnet. Dieser Wert gibt den Anteil der Objekte der positiven Klasse an, der durch ein \acrshort{ml}-Modell errechnet wurde. Der F\textsubscript{1}-Wert hat einen Wert zwischen [0, 1]. Je höher dieser Wert ist, desto
besser ist die Leistung des \acrshort{ml}-Modells~\cite{evaluationsMethoden}.

\end{comment}
