\chapter{Related Work}

\todo{introduce in related work, differentiate the importance of datasets}
Describe relevant scientific literature related to your work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Datasets of UI Trees}
\label{sec:datasets-of-ui-trees}

Requirements on a good data set
- Google and Samsung
- need to have correct data
- needs enough data to train a NN
- need enough features to be able to recognize patterns
- up to date
- Publicly available
- Variable length of app sessions, define one session of activating the screen until it is turned off.

Missing
- System to feed in in real time
- Dataset which is across multiple apps, also tracks the system
-

\subsection{ERICA}
\label{subsec:erica}

ERICA is a design and interaction mining application, which allows gathering \ti{interaction traces} by capturing the users activity on Android apps~\cite{deka2016erica}.
This is accomplished through a web-based interaction layer in contrast to the other common approach of using \ti{accessibility services} directly.
They justify that approach by the lack of need to install additional applications, as only a browser is required.
A further reason is the response latency of the commonly used \ti{UiAutomator}, which cannot collect the data in time.
Also they argue that capturing and simultaneously interacting with the apps may overload the user device and challenges the user experience.
Therefore the much more powerful servers take the task of capturing the UI trees.
The apps are hosted on multiple physical devices with a modified Android OS directly connected with the server.
ERICA captures UI screens and user flows by tracking UI changes.
They then used this data to form k-mean clusters from the UI elements (visual and textual features) and the interactive elements (icons and buttons).
Based on the clusters they then build classifiers and trained an AutoEncoder (\ref{subsubsec:autoencoder}) to determine the flows from the test dataset.
The authors worked out 23 common user flows (from over a thousand popular Android apps) which aim to provide complementary, promising or new design patterns and trends.

%- data-driven app design application
%- gathers user interaction trace > 1000 popular apps
%- 3000 flow examples

%\todo{descibe how they worked out the 23 user flows, which Autoencoder they used etc.}

\subsection{Rico / RicoSCA}

Rico \cite{deka2017rico} (spanish for \quotes{rich}) is the successor of ERICA.
It aims to help perform better at designing and support the creation of adaptive UIs.
As far as known to date this is the largest collection of mobile app designs and traces with covering 72k UI screens in 9.7k Android apps.
Like its predecessor Rico uses a web-based approach to collect user traces.
It enables the applications like searching for designs, generation of UI layouts and code, modeling of user interactions, and prediction of user perception.
It exposes visual, textual, structural, and interactive design properties of more than 72k unique UI screens.
Unfortunately the dataset doesn't include interaction traces for app to app transitions or interactions with the Android OS itself.
In table~\ref{tab:rico_view_hierarchy_attributes} a collection of all view hierarchy attributes is shown with their meaning.
These were extracted by iterating over all view hierarchy files contained in the traces of the dataset.
This gives insights in what attributes were recorded in the Rico dataset and what relevance they may have during training the model.
The authors of Rico used their dataset to train a 64-dimensional UI layout vector~\ref{subsubsec:embedding} with an AutoEncoder \ref{subsubsec:autoencoder}.
For their input they converted the UI layout hierarchy to an image with colored bounding boxes differentiating images and text.
This has the advantage to be able to deal with the high dimensions inside the UI tree.
But the conversion also most likely discards lots of meaningful information hidden in the UI tree semantics.

\begin{table}
  \small
  \centering
  \begin{tabular}{|l|c|c|>{\RaggedRight}p{0.5\linewidth}|}
    \hline
    \tb{Key} & \textbf{Type} & \textbf{Shape} & \textbf{Description} \\
    \hline
    \multicolumn{4}{c}{Per View} \\
    \hline
% Annotated by word2vec
%    \_is\_leaf\_node & bool & (1) & \\
%    \_caption\_preorder\_id & bool & (1) & \\
%    \_caption\_depth & bool & (1) & \\
%    \_caption\_node\_id & bool & (1) & \\
%    \_caption\_postorder\_id & bool & (1) & \\
    activity\_name & string & (1) & Name of the activity: e.g. \quotes{com.my\_app.AppName.MainActivity} \\
%    added\_fragments & [] & (None) & \\
%    active\_fragments & [] & (None) & \\
    is\_keyboard\_deployed & bool & (1) & Indicates if the keyboard is shown \\
    request\_id & int & (1) & Id used by the crawler to request the view \\
    \hline
    \multicolumn{4}{c}{Per Node} \\
    \hline
    abs-pos & bool & (1) & Indicates if position in \ti{bounds} is relative or absolute; if \ti{true}, \ti{rel-bounds} is set \\
    adapter-view & bool & (1) & Indicates that children are loaded via an adapter, see~\cite{android_adapterview} \\
    ancestors & [string] & (None) & Ancestors of current node, e.g. \quotes{android.view.View} \\
    bounds & [integer] & (4) & Absolute or relative boundaries, dependent on \ti{abs-pos} \\
    children & [node] & (None) & Child nodes \\
    class & string & (1) & \quotes{com.my\_app.lib.ui.views.DropDownSpinner} \\
    clickable & bool & (1) & User can interact by press / click \\
    content-desc & string & (1) & (Accessibility) description of the node \quotes{Interstitial close button} \\
    draw & bool & (1) & Indicates if this node is drawn on the canvas \\
    enabled & bool & (1) & Indicates if this node is in the enabled state \\
    focusable & bool & (1) & Indicates if this node can be focused \\
    focused & bool & (1) & Indicates if this node can is currently in focus \\
    font-family & string & (1) & States the font family, e.g. \quotes{sans-serif} \\
    long-clickable & bool & (1) & Indicates if this node has a long press action \\
    package & string & (1) & States which packages the node belongs to \quotes{com.my\_app.mypackage} \\
%    pointer & string & (1) & \todo{Presumably the saving address in the memory, e.g. \quotes{92690f4}} \\
    pressed & bool & (1) & Indicates if this node can is currently pressed \\
    rel-bounds & [integer] & (4) & Relative boundaries, if \ti{abs-pos} is set to \ti{true} \\
    resource-id & string & (1) & The unique resource identifier for this view \quotes{android:id/navigationBarBackground} \\
    scrollable-horizontal & bool & (1) & Indicates if this node can be scrolled horizontally \\
    scrollable-vertical & bool & (1) & Indicates if this node can be scrolled vertically \\
    selected & bool & (1) & Indicates if this node can is currently selected \\
    text & string & (1) & Text value if this node is a textual element \\
    text-hint & bool & (1) & Explanation text for text boxes or icons \\
    visibility & string & (1) & Indicates if this node is hidden, e.g. \quotes{visible}, \quotes{gone}\\
    visible-to-user & bool & (1) & Indicates if this node can be seen in the viewport by the user \\
    \hline
  \end{tabular}
  \caption[Attributes of a view hierarchy record]{Collection of attributes of a \ti{view hierarchy} record, extracted from all interaction traces of the Rico \cite{deka2017rico} dataset.}
  \label{tab:rico_view_hierarchy_attributes}
\end{table}

The RicoSCA dataset has been formed out of the research topic of mapping language instructions to mobile UI action sequences~\cite{li2020mapping}.
They removed screens whose bounding boxes in the view hierarchies are inconsistent with the screenshots with the help of annotators.
The process of filtering reduced the Rico dataset to 25k more concise and meaningful screens.

\subsection{Mobile UI CLAY Dataset}

The Google researchers Gang Li et al. \cite{clay} present a a so-called \ti{CLAY} pipeline which is able to denoise mobile UI layouts from incorrect nodes or adding further semantics to it.
As basis they used the Rico \ref{rico} dataset for a subject of improvement.
They state that recording results are dynamic and can get out of sync with the actual screen of the user.
That leads to 37.4\% of screens which contain invalid objects.
This induces invisible or misaligned objects, or objects which are not clickable (greyed out).
The researchers filtered invalid objects by training a \gls{resnet} model with the screenshots to classify nodes as as invalid if their bounding boxes don't match.
Also they introduced two models: a \gls{gnn} and a Transformer model to each determine the view type (also related to the view class).
For that they considered the view hierarchy attributes as well as the screenshots via a \gls{cnn}.
They claim they outperform heuristic approaches for detecting layout objects without a visual valid counterpart and also can recognize their types in more than 85\%.
This pipeline could help to improve intent prediction algorithms as less inconsistent data is applied to the model.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Vector models}

- Compress a huge data set to a concise model
- Vector Representation enables

Advantages:
- \quotes{small} or smaller than the data set itself
- No need to have pre knowledge about the topic, just need input an output (labels) for unsupervised NN
-

\subsection{Doc2Vec and Word2Vec}
%http://proceedings.mlr.press/v32/le14.pdf
%6_Quoc-Le_Doc2Vec
\cite{le2014distributed}

\subsection{Screen2Vec}
\cite{li2021screen2vec}

\subsection{Screen2Words}

\subsection{Intention2Text}
\cite{yu2020understanding}

\subsection{Html2Vec}
\cite{wu2022distributed}

\subsection{Tree2Vec}

\subsection{Activity2Vec}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Time Series / Sequence models}

- one more dimension
- allows predicting unseen states
- back propagation -> see technical part
-

RNNs: 9\_Personalizing session based recommendations with RNNs \cite{quadrana2017personalizing}
10\_Bansal\_Hybrid RNN Recommender system \cite{bansal2022remembering}
\cite{pietro2022recommendationSystems}

\subsection{Seq2Seq Model}
\cite{chollet2017seq2seq}

\subsection{Click Sequence Prediction / PathFinder}

Seokjun Lee et al. \cite{lee2018click} propose a technique called \ti{PathFinder} which aims to predict the sequence of user clicks in Android mobile apps.
The user input and the contextual data is collected via the Android Accessibility Services \ref{accessibility_services}, so the users \gls{os} does not need to be modified or \glslink{gl-rooting}{rooted}.
They collected the data from 55 students of their university with a sequence tracing tool and collected near 2 million button clicks from over a thousand apps.
They follow a collaborative and content-based approach which takes both all the users data as well as the individual preferences into account.
The \ti{button depth} describes the number of clicks or taps until the user gets to their target screen.
In average nearly the user has 16 buttons as candidates to press as the next action.
With a personalized UI a the \ti{button depth} should decrease significantly.
The next user click is dependent on very recent but also on previous clicks happened a longer time ago, e.g.~taking a picture relates to uploading it later to their \gls{sns}.
The authors train a \gls{lstm} model to predict the next button, which will be clicked on.
PathFinder predicts the most probable three buttons with a 0.76 F-measure.

In contrast to this work, \ti{PathFinder} does not take into account the complete view hierarchy or other spatiotemporal information.
Just the previous and the current app and the click history with their button properties are considered.
Also as far as known the dataset and code is not publicly accessible.

% "Very challenging to make accurate predictions of the click sequence."

\subsection{Large-Scale Modeling of Mobile User Click Behaviors Using Deep Learning}
\cite{zhou2021large}
-> No code or dataset

- understand interaction behavior
- ui optimization
- recommending next element to click on
- dataset of 20m clicks from 4k mobile users
- Based on \ref{tranformer} model
- deep leraning model
- 48\% \ref{accuracy} and 71\% accuracy for top-3
- challenges: accurate and scalabele click sequence modeling:
  - exceeding number of unique UI screens in different apps -> no predefined set of UI elements
  - users click behavior is very individual and heavily dependent on situational factors
- tackles cross-app transitions (26\% of all clicks)
- differentiate \ti{actionable} UI elements, if they are \code{clickable}, \code{visible} and \code{enable} (cf. \ref{tab:rico_view_hierarchy_attributes})
- takes current context into account, time of the day, day in the week
- uses context of the screen (bounding boxes, text content, type) and also click-sequence

\todo{describe and link figure}

\begin{figure}[htbp!]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphics/zhou_actionable_elements}
    \caption{Number of actionable elements per screen}
    \label{fig:zhou_actionable_elements}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphics/zhou_event_time_intervals}
    \caption{The distribution of time intervals between click events}
    \label{fig:zhou_event_time_intervals}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{graphics/zhou_event_time_distribution}
    \caption{The distribution of time intervals between click events}
    \label{fig:zhou_event_time_distribution}
  \end{subfigure}
  \caption{View element insights \cite{zhou2021large}}
  \label{fig:zhou_graphs}
\end{figure}
